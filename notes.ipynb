{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The CKY Algorithm\n",
    "\n",
    "A context-free grammar parser using the CYK algorithm (and my notes) as described in [Chapter 17](https://web.stanford.edu/~jurafsky/slp3/17.pdf) of the [Speech and Language Processing textbook](https://web.stanford.edu/~jurafsky/slp3/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Context-Free Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A context-free grammar $G$ is defined by four parameters\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "$N$ - a set of non-terminal symbols (or variables)  \n",
    "\n",
    "$Î£$ - a set of terminal symbols (disjoint from $N$)  \n",
    "\n",
    "$R$ - a set of rules or productions, each of the form $(A \\rightarrow \\beta)$,  \n",
    "where $A$ is a non-terminal, $\\beta$ is a string of symbols from the infinite set of strings $(\\Sigma \\cup N)^*$\n",
    "\n",
    "$S$ - a designated start symbol and a member of $N$\n",
    "\n",
    "</div>\n",
    "\n",
    "Capital letters refer to non-terminals. $S$ is the start symbol. Lowercase Greek letters like $\\alpha$ and $\\gamma$ are strings drawn from $(\\Sigma \\cup N)^*$. Lowercase Roman letters like $u$ and $w$ represent the strings of terminals.\n",
    "\n",
    "Let's take a moment to figure out what $(\\Sigma \\cup N)^*$ means.\n",
    "\n",
    "In the context of parsing text, the non-terminals represent phrasal categories (e.g., noun phrase, verb phrase), while the terminals represent actual words or tokens in the text. The terminals are typically preceded by pre-terminals that correspond to the parts of speech of the terminal word. \n",
    "\n",
    "\n",
    "```text\n",
    "Constituency tree for \"I prefer a morning flight\"\n",
    "\n",
    "         S\n",
    "       /   \\\n",
    "     NP     VP\n",
    "     |    /    \\\n",
    "   Pro  Verb    NP\n",
    "    |    |      /  \\\n",
    "    I  prefer Det  Nom\n",
    "               |   /  \\\n",
    "               a  Nom  Noun\n",
    "                  |      |\n",
    "                 Noun   flight\n",
    "                  |\n",
    "                morning\n",
    "```\n",
    "\n",
    "```text\n",
    "[S[NP[Pro \"I\"]][VP[Verb \"prefer\"][NP[Det \"a\"][Nom[Nom[Noun \"flight\"]][Noun \"morning\"]]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Note:** The Kleene star operation, denoted with an asterisk $*$, is applied to a set of strings, and represents the set of all possible strings that can be formed by concatenating zero or more strings from the original set.\n",
    "\n",
    "For example, suppose we have the set $S = \\{a, b\\}$. Then \n",
    "\n",
    "$$S^* = \\{\\epsilon, a, b, ab, ba, aab, aba, \\dots \\}$$\n",
    "\n",
    "where $\\epsilon$ is the empty string.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the Grammar and Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMMAR_DIR = \"grammar\"\n",
    "LEXICON_DIR = \"lexicon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(language):\n",
    "    # get the content of the grammar and lexicon files\n",
    "    GRAMMAR_FILE = os.path.join(GRAMMAR_DIR, f\"{language}.txt\")\n",
    "    LEXICON_FILE = os.path.join(LEXICON_DIR, f\"{language}.txt\")\n",
    "    with open(GRAMMAR_FILE, \"r\") as f: grammar_text = f.read()\n",
    "    with open(LEXICON_FILE, \"r\") as f: lexicon_text = f.read()\n",
    "    grammar_lines = grammar_text.strip().split(\"\\n\")\n",
    "    lexicon_lines = lexicon_text.strip().split(\"\\n\")\n",
    "    return grammar_lines, lexicon_lines\n",
    "\n",
    "def parse_rules(language):\n",
    "    # parse the grammar and lexicon rules\n",
    "    # might as well combine them to make them easier to work with\n",
    "    grammar_lines, lexicon_lines = read_files(language)\n",
    "    lines = grammar_lines + lexicon_lines\n",
    "    \n",
    "    rules = []\n",
    "\n",
    "    for line in lines:\n",
    "        # grammar format: 'A' -> 'B' 'C'\n",
    "        # lexicon format: 'W' -> 'w1' | 'w2 w3' | 'w4'\n",
    "        lhs, rhs = line.split(\"->\")\n",
    "\n",
    "        if \"|\" in rhs:\n",
    "            # lexicon\n",
    "            vocabs = rhs.split(\"|\")\n",
    "            # convert all vocab to lowercase\n",
    "            # this will resolve the issue of distinguishing between\n",
    "            # non-terminals (uppercase) and terminals (lowercase)\n",
    "            # the letter I or city Houston starts with a capital\n",
    "            # and will make the algo think they are non-terminals\n",
    "            rhs = [vocab.strip().lower() for vocab in vocabs]\n",
    "        else:\n",
    "            # grammar\n",
    "            phrases = rhs.split()\n",
    "            rhs = [phrase.strip() for phrase in phrases]\n",
    "\n",
    "        rules.append((lhs.strip(), rhs))\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"L0\"\n",
    "grammar = parse_rules(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', ['NP', 'VP']),\n",
       " ('NP', ['Proper-Noun']),\n",
       " ('Nominal', ['Nominal', 'PP']),\n",
       " ('VP', ['Verb', 'PP']),\n",
       " ('Noun', ['book', 'flight', 'meal', 'money']),\n",
       " ('Aux', ['does'])]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar[::4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Chomsky Normal Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the CYK algorithm to work, we need to convert the grammars to the Chomsky Normal Form (CNF). A CFG is in CNF if it has rules of the form\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**1.** $A \\rightarrow B \\  C$\n",
    "\n",
    "**2.** $A \\rightarrow a$\n",
    "\n",
    "**3.** no $A \\rightarrow \\epsilon$ productions ($\\epsilon$-free)\n",
    "\n",
    "</div>\n",
    "\n",
    "where uppercase letters are non-terminals and lowercase letters are terminals. So the rules can expand to either two non-terminals or a single terminal. This means that Chomsky normal form grammars are binary trees down to the pre-lexical node (POS of word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assume that we're dealing with $\\epsilon$-free grammar, so we can ignore condition (3). Then there are three situations that need to be addressed:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**1.**\n",
    "**Rules that mix terminals with non-terminals on the right-hand side**\n",
    "\n",
    "These can be normalized by introducing dummy non-terminals that map the original terminal So the production \n",
    "$$A \\rightarrow B \\ c$$  \n",
    "\n",
    "becomes\n",
    "$$A \\rightarrow B \\ C$$\n",
    "$$C \\rightarrow c$$\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**2.** \n",
    "**Rules that have a single non-terminal on the right-hand side (unit productions)**\n",
    "\n",
    "Unit productions can be removed by rewriting the right-hand side of the original rules with the right-hand side of all the non-unit production rules that they ultimately lead to. Formally, if $A \\overset{*}{\\Rightarrow} B$ by a chain of one or more unit productions and $B \\rightarrow \\gamma$ is a non-unit production in our grammar, then we add $A \\rightarrow \\gamma$ for each such rule in the grammar and discard all intervening productions.\n",
    "\n",
    "For a more concrete example, suppose we have rules\n",
    "\\begin{align*}\n",
    "S &\\rightarrow A \\\\\n",
    "A &\\rightarrow B \\\\\n",
    "B &\\rightarrow a \\ B \\ b \\\\\n",
    "B &\\rightarrow \\epsilon \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then the first two rules are unit productions. We can see that $A$ can derive $B$, and $S$ can derive $B$ through $A$. This means that both $S$ and $A$ are able to derive strings that $B$ can derive, so we add the following rules:\n",
    "\\begin{align*}\n",
    "S &\\rightarrow a \\ B \\ b \\\\\n",
    "S &\\rightarrow \\epsilon \\\\\n",
    "A &\\rightarrow a \\ B \\ b \\\\\n",
    "A &\\rightarrow \\epsilon \\\\\n",
    "\\end{align*}\n",
    "\n",
    "and remove the original unit production rules from $S$ and $A$ to create a new grammar\n",
    "\\begin{align*}\n",
    "S &\\rightarrow a \\ B \\ b \\\\\n",
    "S &\\rightarrow \\epsilon \\\\\n",
    "A &\\rightarrow a \\ B \\ b \\\\\n",
    "A &\\rightarrow \\epsilon \\\\\n",
    "B &\\rightarrow a \\ B \\ b \\\\\n",
    "B &\\rightarrow \\epsilon \\\\\n",
    "\\end{align*}\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**3.**\n",
    "**Rules in which the length of the right-hand side is greater than two**\n",
    "\n",
    "These can be normalized through the introduction of new non-terminals that spread the longer sequences over several new rules. For example, if we have\n",
    "$$A \\rightarrow B \\ C \\ \\gamma$$\n",
    "\n",
    "we replace the leftmost pair of non-terminals with a new non-terminal and introduce a new production rule:\n",
    "$$A \\rightarrow X1 \\ \\gamma$$\n",
    "$$X1 \\rightarrow B \\ C$$\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNF conversion algorithm runs as follows:\n",
    "\n",
    "1) copy all conforming rules to the new grammar unchanged\n",
    "    - a `grammar` is passed into the `to_cnf` function\n",
    "    \n",
    "    - the `grammar` is a list of productions of the form (LHS, [RHS_1, RHS_2, ...])\n",
    "\n",
    "2) convert terminals within rules to dummy non-terminals\n",
    "    - the results are stored in `new_productions` \n",
    "\n",
    "    - if the RHS of a rule has only non-terminals $\\left(A \\rightarrow B \\ C \\right)$, it is copied over into `new_productions` \n",
    "    \n",
    "    - if the RHS includes a non-terminal $\\left(A \\rightarrow b \\ C \\right)$, a dummy rule $\\left(B \\rightarrow b \\right)$ is created, the RHS of the previous rule is updated $\\left(A \\rightarrow B \\ C \\right)$, and both of these new rules are added to `new_productions`\n",
    "\n",
    "3) make all rules binary and add them to the new grammar\n",
    "    - for each rule in `new_productions`, we will reduce the size of the RHS to two non-terminals (we only check for productions with RHS greater than two, so the non-terminal-to-terminal mappings and the unit productions will remain unchanged)\n",
    "\n",
    "    - dummy non-terminals with the name $X$ replace the first two non-terminals in rules with a RHS with more than 2 non-terminals, e.g., $A \\rightarrow B \\ C \\ D$ becomes $A \\rightarrow X1 \\ D$ and $X1 \\rightarrow B \\ C$\n",
    "\n",
    "    - this step is repeated for each rule until the RHS contains only two non-terminals -- we store the new productions in `binary_productions`\n",
    "\n",
    "4) convert unit productions\n",
    "    -   at this point, all productions in `binary_productions` will either produce \n",
    "        \n",
    "        1) binary non-terminals: $A \\rightarrow B \\ C$ \n",
    "\n",
    "        2) unit productions: $A \\rightarrow B$ \n",
    "\n",
    "        3) mappings to terminals $A \\rightarrow a$ \n",
    "\n",
    "    - we need to remove the unit productions, which are stored in `unit_productions`\n",
    "\n",
    "    -  sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(symbol):\n",
    "    # we assume that terminal nodes are all lowercase\n",
    "    # and non-terminal nodes start with an uppercase letter\n",
    "    return symbol[0].islower()\n",
    "\n",
    "def to_cnf(grammar):\n",
    "    '''grammar is list of form ('LHS', ['RHS_1', 'RHS_2', ...])'''\n",
    "\n",
    "\n",
    "    ''' 1) Rules that mix terminals with non-terminals on the right-hand side'''\n",
    "    # remove terminals from rhs of production by creating non-terminal mappings\n",
    "    # e.g, turn (A -> b C) into (A -> B C and B -> b)\n",
    "    # this section does not modify rules that do not contain rhs terminals \n",
    "    #  simply copies them over from grammar to new_productions (A -> B) remains (A -> B)\n",
    "    terminal_mappings = {} # store the non-terminal-to-terminal mappings: B -> b\n",
    "    new_productions = []   # store the updated production rules in form ('LHS', ['RHS_1', 'RHS_2', ...])\n",
    "\n",
    "    for lhs, rhs in grammar:\n",
    "        new_rhs = []\n",
    "        for symbol in rhs:\n",
    "            # iterate over the (non-)terminals on the rhs \n",
    "            if is_terminal(symbol):\n",
    "                if symbol not in terminal_mappings:\n",
    "                    # convert to non-terminal by uppercasing the word\n",
    "                    non_terminal = symbol.upper()\n",
    "                    terminal_mappings[symbol] = non_terminal\n",
    "                    # add the new rule: A -> a\n",
    "                    new_productions.append((non_terminal, [symbol]))\n",
    "                new_rhs.append(terminal_mappings[symbol])\n",
    "            else:\n",
    "                # if it is a non-terminal, do nothing\n",
    "                new_rhs.append(symbol)\n",
    "\n",
    "        new_productions.append((lhs, new_rhs))\n",
    "\n",
    "\n",
    "    ''' 2) Rules in which the length of the right-hand side is greater than two'''\n",
    "    # we will binarize productions before removing unit productions\n",
    "    # this means that we will reduce the size of the RHS from \n",
    "    # len(rhs) > 2 to len(rhs) <= 2 \n",
    "    binary_productions = [] # store the updated production rules\n",
    "    counter = 1             # counter is used for creating dummy non-terminals\n",
    "\n",
    "    for lhs, rhs in new_productions:\n",
    "        while(len(rhs) > 2):\n",
    "            # create the dummy terminal \n",
    "            new_non_terminal = f\"X{counter}\"\n",
    "            counter += 1\n",
    "            # add the dummy rule: X# -> [first two vals on rhs]\n",
    "            binary_productions.append((new_non_terminal, rhs[:2]))\n",
    "            # update the rhs: rhs = [X#, everything but first 2 vals on rhs]\n",
    "            rhs = [new_non_terminal] + rhs[2:]\n",
    "        # once the rule has been binarized, or if there is no issue, add it\n",
    "        binary_productions.append((lhs, rhs))\n",
    "\n",
    "\n",
    "    ''' 3) Rules that mix terminals with non-terminals on the right-hand side'''\n",
    "    # at this point, we have binary non-terminal productions, unit productions\n",
    "    # and mappings to terminals -- we need to remove the unit productions\n",
    "    unit_productions = []\n",
    "    for prod in binary_productions:\n",
    "        lhs, rhs = prod\n",
    "        # rhs has from [rhs_1, rhs_2, ...]\n",
    "        # check if rhs has single element, and if that element\n",
    "        # is not a terminal, e.g., rhs = [A] --> rhs[0] = A is not a terminal\n",
    "        if len(rhs) == 1 and not is_terminal(rhs[0]):\n",
    "            unit_productions.append(prod)\n",
    "\n",
    "    while unit_productions:\n",
    "        # remove unit prods until they are all gone\n",
    "        # e.g., lhs == A, rhs == [B]\n",
    "        unit_lhs, unit_rhs = unit_productions.pop()\n",
    "\n",
    "        # find all of the possible chains from the rhs of a unit \n",
    "        # production into a binary rule and store the rhs of the binary rule\n",
    "        # e.g. \n",
    "        #   unit_prod: (A, [B])\n",
    "        #   bnry_prod: (B, [C, D])\n",
    "        #   [C,D] is stored because bnry_prod[0] == B == unit_prod[1][0] == B\n",
    "        chains = []\n",
    "        for bnry_prod in binary_productions:\n",
    "            bnry_lhs, bnry_rhs = bnry_prod # lhs = B, rhs = [C, D]\n",
    "            if bnry_lhs == unit_rhs[0]:    # if B == [B][0] == B\n",
    "                chains.append(bnry_rhs)    # store [C, D]\n",
    "        \n",
    "        for new_rhs in chains:\n",
    "            if len(new_rhs) == 1 and not is_terminal(new_rhs[0]):\n",
    "                # might create more unit prods in process of removing them -- add to list to be removed\n",
    "                # e.g., unit_prod: (A, [B]), bnry_prods: (B, [C]) and (C, [D, E])\n",
    "                # we first create (A, [C]) (add to list), and then create (A, [D, E])\n",
    "                unit_productions.append((unit_lhs, new_rhs))\n",
    "            else:\n",
    "                # if the result is not a unit prod, then it is complete!\n",
    "                binary_productions.append((unit_lhs, new_rhs))\n",
    "        \n",
    "        # filter the current unit prod from the binary prods list\n",
    "        binary_productions = [prod for prod in binary_productions if prod != (unit_lhs, unit_rhs)]\n",
    "        \n",
    "    return binary_productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_grammar = to_cnf(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', ['NP', 'VP']),\n",
       " ('X1', ['Aux', 'NP']),\n",
       " ('S', ['X1', 'VP']),\n",
       " ('NP', ['Det', 'Nominal']),\n",
       " ('Nominal', ['Nominal', 'Noun']),\n",
       " ('Nominal', ['Nominal', 'PP']),\n",
       " ('VP', ['Verb', 'NP']),\n",
       " ('X2', ['Verb', 'NP']),\n",
       " ('VP', ['X2', 'PP']),\n",
       " ('VP', ['Verb', 'PP']),\n",
       " ('VP', ['VP', 'PP']),\n",
       " ('PP', ['Preposition', 'NP']),\n",
       " ('THAT', ['that']),\n",
       " ('THIS', ['this']),\n",
       " ('THE', ['the']),\n",
       " ('A', ['a']),\n",
       " ('X3', ['THAT', 'THIS']),\n",
       " ('X4', ['X3', 'THE']),\n",
       " ('Det', ['X4', 'A']),\n",
       " ('BOOK', ['book']),\n",
       " ('FLIGHT', ['flight']),\n",
       " ('MEAL', ['meal']),\n",
       " ('MONEY', ['money']),\n",
       " ('X5', ['BOOK', 'FLIGHT']),\n",
       " ('X6', ['X5', 'MEAL']),\n",
       " ('Noun', ['X6', 'MONEY']),\n",
       " ('INCLUDE', ['include']),\n",
       " ('PREFER', ['prefer']),\n",
       " ('X7', ['BOOK', 'INCLUDE']),\n",
       " ('Verb', ['X7', 'PREFER']),\n",
       " ('I', ['i']),\n",
       " ('SHE', ['she']),\n",
       " ('ME', ['me']),\n",
       " ('X8', ['I', 'SHE']),\n",
       " ('Pronoun', ['X8', 'ME']),\n",
       " ('HOUSTON', ['houston']),\n",
       " ('NWA', ['nwa']),\n",
       " ('Proper-Noun', ['HOUSTON', 'NWA']),\n",
       " ('DOES', ['does']),\n",
       " ('FROM', ['from']),\n",
       " ('TO', ['to']),\n",
       " ('ON', ['on']),\n",
       " ('NEAR', ['near']),\n",
       " ('THROUGH', ['through']),\n",
       " ('X9', ['FROM', 'TO']),\n",
       " ('X10', ['X9', 'ON']),\n",
       " ('X11', ['X10', 'NEAR']),\n",
       " ('Preposition', ['X11', 'THROUGH']),\n",
       " ('Aux', ['does']),\n",
       " ('VP', ['X7', 'PREFER']),\n",
       " ('Nominal', ['X6', 'MONEY']),\n",
       " ('NP', ['HOUSTON', 'NWA']),\n",
       " ('NP', ['X8', 'ME']),\n",
       " ('S', ['Verb', 'NP']),\n",
       " ('S', ['X2', 'PP']),\n",
       " ('S', ['Verb', 'PP']),\n",
       " ('S', ['VP', 'PP']),\n",
       " ('S', ['X7', 'PREFER'])]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CKY Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{function} & \\ \\text{CKY-PARSE}(words, grammar) \\ \\text{returns table} \\\\\n",
    "& \\\\\n",
    "& \\text{for } j \\leftarrow 1 \\ \\text{to} \\ \\text{LENGTH}(words) \\ \\text{do} \\\\\n",
    "& \\quad \\text{for all } \\{A \\mid A \\rightarrow words[j] \\in grammar\\} \\\\\n",
    "& \\quad \\quad \\text{table}[j-1,j] \\leftarrow \\text{table}[j-1,j] \\cup A \\\\\n",
    "& \\quad \\text{for } i \\leftarrow j-2 \\ \\text{down to} \\ 0 \\ \\text{do} \\\\\n",
    "& \\quad \\quad \\text{for } k \\leftarrow i+1 \\ \\text{to} \\ j-1 \\ \\text{do} \\\\\n",
    "& \\quad \\quad \\quad \\text{for all } \\{A \\mid A \\rightarrow BC \\in grammar \\ \\text{and} \\ B \\in \\text{table}[i,k] \\ \\text{and} \\ C \\in \\text{table}[k,j]\\} \\\\\n",
    "& \\quad \\quad \\quad \\quad \\text{table}[i,j] \\leftarrow \\text{table}[i,j] \\cup A \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\text{function} & \\ \\text{CKY-PARSE}(words, grammar) \\ \\text{returns table} \\\\\n",
    "& \\\\\n",
    "& \\text{for } j \\leftarrow 1 \\ \\text{to} \\ \\text{LENGTH}(words) \\ \\text{do} \\\\\n",
    "& \\quad \\text{for all } \\{A \\mid A \\rightarrow words[j] \\in grammar\\} \\\\\n",
    "& \\quad \\quad \\text{table}[j-1,j] \\leftarrow \\text{table}[j-1,j] \\cup A \\\\\n",
    "& \\quad \\text{for } i \\leftarrow j-2 \\ \\text{down to} \\ 0 \\ \\text{do} \\\\\n",
    "& \\quad \\quad \\text{for } k \\leftarrow i+1 \\ \\text{to} \\ j-1 \\ \\text{do} \\\\\n",
    "& \\quad \\quad \\quad \\text{for all } \\{A \\mid A \\rightarrow BC \\in grammar \\\\\n",
    "& \\quad \\quad \\quad \\quad \\text{and} \\ B \\in \\text{table}[i,k] \\ \\text{and} \\ C \\in \\text{table}[k,j]\\} \\\\\n",
    "& \\quad \\quad \\quad \\quad \\text{table}[i,j] \\leftarrow \\text{table}[i,j] \\cup A \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CKY_parse(sentence, grammar):\n",
    "    table = []\n",
    "    words = sentence.split()\n",
    "\n",
    "    # j in [1, len(words)]\n",
    "    for j in range(1, len(words)+1):\n",
    "        print(\"j:\", j)\n",
    "\n",
    "        # i in [0, j-2]\n",
    "        for i in range(0, (j-2)+1):\n",
    "            print(\"i:\", i)\n",
    "\n",
    "            # k in [i+1, j-1]\n",
    "            for k in range(i+1, (j-1)+1):\n",
    "                print(\"k:\", k)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Book the flight through Houston\"\n",
    "# table = CKY_parse(sentence, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert terminals within rules to dummy non-terminals.\n",
    "3. Convert unit productions.\n",
    "4. Make all rules binary and add them to new grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_deps(grammar, symbol):\n",
    "    # get dependencies for the symbol \n",
    "    # there should only be one \n",
    "    # the symbol will always be the first element of \n",
    "    # the rhs of the production rule:\n",
    "    #   rule = (lhs, rhs) = (lhs, [symbol, ...])\n",
    "    deps = []\n",
    "    for lhs, rhs in grammar:\n",
    "        if symbol == rhs[0]:\n",
    "            deps.append((lhs, rhs))    \n",
    "    return deps   \n",
    "\n",
    "\n",
    "\n",
    "def unroll(grammar, vocab):\n",
    "    to_change = []\n",
    "    for lhs, rhs in grammar:\n",
    "        first_symbol = rhs[0]\n",
    "        if first_symbol[0] == \"X\":\n",
    "            # add the symbol and rule:\n",
    "            # (symbol, (lhs, rhs)) = (symbol, (lhs, [symbol, ...]))\n",
    "            to_change.append((first_symbol, (lhs, rhs)))\n",
    "\n",
    "    to_remove = []\n",
    "    for symbol, rule in to_change:\n",
    "        # get the dependencies for the current rule\n",
    "        deps = find_deps(grammar, symbol)\n",
    "        # remove the rule and its dependencies from the grammar\n",
    "        to_remove += [rule] + deps\n",
    "\n",
    "        # should only be 1 dep...\n",
    "        for dep in deps:\n",
    "            # rule: (r_lhs, [symbol, ...])\n",
    "            # deps: (symbol, d_rhs)\n",
    "            # final: (r_lhs, [symbol, d_rhs, ...])\n",
    "            rule_lhs, rule_rhs = rule\n",
    "            dep_lhs, dep_rhs = dep\n",
    "            # omit the first element in rule_rhs\n",
    "            rule = (rule_lhs, [dep_rhs, rule_rhs[1:]])\n",
    "            \n",
    "            # add to grammar\n",
    "            grammar += rule\n",
    "    \n",
    "    # return the updated grammar\n",
    "    return [rule for rule in grammar if rule not in to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngoal:\\n    S -> BOOK\\n    S -> PREFER\\n    S -> INCLUDE\\n'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grammar = [('S', ['X7', 'PREFER']),\n",
    "         ('X7', ['BOOK', 'INCLUDE']),\n",
    "         ('PREFER', ['prefer']),\n",
    "         ('INCLUDE', ['include']),\n",
    "         ('BOOK', ['book'])]\n",
    "\n",
    "vocab = [\"prefer\", \"include\"]\n",
    "\n",
    "'''\n",
    "goal:\n",
    "    S -> BOOK\n",
    "    S -> PREFER\n",
    "    S -> INCLUDE\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X7', ['BOOK', 'INCLUDE']),\n",
       " ('PREFER', ['prefer']),\n",
       " ('INCLUDE', ['include']),\n",
       " ('BOOK', ['book']),\n",
       " 'S',\n",
       " [['X7', 'PREFER'], ['PREFER']]]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unroll(grammar, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', ['book', 'PREFER']),\n",
       " ('X7', ['book', 'INCLUDE']),\n",
       " ('PREFER', ['prefer']),\n",
       " ('INCLUDE', ['include']),\n",
       " ('BOOK', ['book'])]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_terminal_expansions(symbol, grammar):\n",
    "    expansions = set()\n",
    "    for lhs, rhs in grammar:\n",
    "        if lhs == symbol:\n",
    "            if is_terminal(rhs[0]):\n",
    "                expansions.add(rhs[0])\n",
    "            else:\n",
    "                expansions |= get_all_terminal_expansions(rhs[0], grammar)\n",
    "    return expansions\n",
    "\n",
    "def unroll(grammar, vocab):\n",
    "    new_productions = []\n",
    "    for lhs, rhs in grammar:\n",
    "        if is_terminal(rhs[0]) or len(rhs) == 1:\n",
    "            new_productions.append((lhs, rhs))\n",
    "        else:\n",
    "            terminal_expansions = get_all_terminal_expansions(rhs[0], grammar)\n",
    "            for term in terminal_expansions:\n",
    "                new_rule_rhs = [term] + list(rhs[1:])\n",
    "                new_productions.append((lhs, new_rule_rhs))\n",
    "    return new_productions\n",
    "\n",
    "grammar = [('S', ['X7', 'PREFER']),\n",
    "           ('X7', ['BOOK', 'INCLUDE']),\n",
    "           ('PREFER', ['prefer']),\n",
    "           ('INCLUDE', ['include']),\n",
    "           ('BOOK', ['book'])]\n",
    "\n",
    "vocab = [\"prefer\", \"include\"]\n",
    "\n",
    "unroll(grammar, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap4641",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
